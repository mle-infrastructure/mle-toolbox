{
"train_config": {"agent_name": "PPO",         # Class of agents to train
                 "seed_id": 1,                # Training seed for init
                 "num_env_steps": 1024,  # Batch updates to train with
                 "env_name": "Pendulum-v0",   # Name of training environment
                 "device_name": "cpu",        # Device on which to run sim
                 # Optimization-spec. hyperparameters
                 "opt_type": "Adam",
                 "l_rate":  1e-3,
                 "batch_size": 256,           # Divides by threads for sync
                 "num_threads": 2,           # Threads
                 "steps_in_trajectory": 128,  # Collect 16x128 transitions
                 "optimization_epochs": 5,   # Thread rollout
                 # PPO-spec. hyperparams (policy type)
                 "ppo_clip_param": 0.2,
                 "return_type": "gae",                # vanilla/gae
                 "gae_tau": 0.95,
                 "shared_torso": 0,
                 "entropy_beta": 0.001,
                 "value_beta": 0.5,
                 "clip_action_to_range": 0,
                 "policy_type": "gaussian",
                 "recurrent_policy": 0,
                 # MDP-spec. hyperparams (Steps, Discount)
                 "max_steps_in_episode": 200,
                 "train_discount_factor": 0.99,
                 "test_discount_factor": 1,
                 # Logging-spec. hyperparams (When/how much)
                 "num_test_episodes": 10,
                 "evaluate_every_env_steps": 100
                },
"log_config": {"time_to_track": ["optim_counter",
                                "step_counter", "t_since_last_opt"],
               "what_to_track": ["rew_mean", "rew_sd", "rew_median",
                                 "rew_10th_p", "rew_90th_p",
                                 "steps_mean", "steps_sd", "steps_median",
                                 "steps_10th_p", "steps_90th_p",
                                 "success_rate", "loss"],
               "time_to_print": ["optim_counter", "t_since_last_opt"],
               "what_to_print": ["rew_mean", "rew_median",
                                 "steps_mean", "steps_median",
                                 "success_rate", "loss"],
               "print_every_k_updates": 1,
               "overwrite_experiment_dir": 1},
"net_config": {# Specify the actor architecture
                "policy": {"input_dim": [1, 3],
                           "layers_info": [["flatten"],
                                          ["linear", 32, 1],
                                          ["linear", 1, 1]],
                            "output_act": "identity",
                            "hidden_act": "relu",
                            "learn_constant": 1    # whether to learn log std
                        },
                # Specify the critic architecture
                "value": {"input_dim": [1, 3],
                           "layers_info": [["flatten"],
                                          ["linear", 32, 1],
                                          ["linear", 1, 1]],
                            "output_act": "identity",
                            "hidden_act": "relu",
                            "dropout": 0.0,
                            "batch_norm": 0}}
}
